{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic array resizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So far, we've covered static arrays, where we preallocate a fixed amount of memory to create the array of fixed length\n",
    "\n",
    "- Depending on the language, of course, arrays are often not restricted to fixed lengths\n",
    "    - `list` in python, for example, allows for indefinite `append`s\n",
    "\n",
    "- Under the hood, however, this is simply a resizing of the array on the fly to create a **dynamic array**\n",
    "    - We store a pointer to an array with capacity 2\n",
    "    - When we try to do a push when the first array is at capacity, a new array is created with double the capacity\n",
    "    - Copy the elements of array 1 to array 2\n",
    "    - Update pointer to array 2\n",
    "\n",
    "- Dynamic array methods\n",
    "    - `get(i)`: O(1) \n",
    "        - return arr[i]\n",
    "    - `set(i, x)`: O(1) \n",
    "        - arr[i] = x\n",
    "    - `pushback(x)`: O(N) if at capacity, else O(1)\n",
    "        - if array.size() == capacity: allocate new array --> copy old array into new --> update pointer to new array --> update capacity to capacity * 2\n",
    "        - push x into position i = array.size\n",
    "        - update size = size + 1\n",
    "    - `remove(i)`: O(N)\n",
    "        - For values j from position i to size-2 --> del array[i] --> array[i] = array[i+1]\n",
    "    - `size():` O(1)\n",
    "        - return size\n",
    "\n",
    "- Note that in this manner of growing the array, at worst half the space is wasted on average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amortized analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The main objective of discussing them was to discuss the strange time complexity of the `pushback` function. Notice that the time complexity of this function is not definite! \n",
    "    - It is correct to say that it is $O(N)$, because you have to expand the array in the worst case\n",
    "    - But also notice that you don't incur $O(N)$ each time! In fact, you only incur $O(N)$ when the array is at specific points $2^1, 2^2, 2^3,...2^n$\n",
    "\n",
    "- Therefore, it is **typically** inaccurate to say that pushback is $O(N)$; because of this behaviour, the linear time behaviour only happens at intervals! \n",
    "    - In such cases, we look at what typically happens, rather than purely worst case projections, because it may be misleading\n",
    "    - This complexity analysis known as **amortized analysis**\n",
    "\n",
    "- Formally, we define `amortized cost` to be $$\\frac{\\text{Cost(n operations)}}{n}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Amortized Cost: Aggregate Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's consider the `PushBack` method in the dynamic array\n",
    "    - Let's suppose we make $n$ calls to `PushBack`\n",
    "    - Let's suppose the $i$-th insertion costs $c_i$. $c_i$ has a consistent insertion cost of 1, plus an extra cost of shuffling the array if it is a power of 2:\n",
    "    $$c_i = 1 + \\begin{Bmatrix} i-1 & \\text{if i-1 is a power of 2} \\\\ 0 & \\text{otherwise} \\end{Bmatrix}$$\n",
    "    \n",
    "- Recall that the amortized cost is $\\frac{\\text{Cost(n operations)}}{n}$ So in this case\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\frac{\\text{Cost(n operations)}}{n} &= \\frac{\\sum_{i=1}^{n} c_i}{n} \\\\\n",
    "    &= \\frac{n + \\sum_{j=1}^{\\left \\lfloor \\log_2(n-1) \\right \\rfloor} 2^j}{n} \\\\\n",
    "    &= \\frac{O(N)}{N} \\\\\n",
    "    &= O(1)\n",
    "\\end{aligned}$$\n",
    "\n",
    "- What is the meaning of $\\log_2(n-1)$? This just computes the number of times we hit a multiple of 2 when we have $n$ requests!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Amortized Cost: Banker's method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I don't really like this method, because it's not clear what we should use for the \"extra\" charge. In any case, the intuition is exactly the same as the aggregate method\n",
    "\n",
    "- Every time you push a token into the array, you place a token on itself, and a token on 1 element prior to it. \n",
    "    - in this case, you place it on size/2\n",
    "\n",
    "- When we reach a point of doubling such that $n -> 2n$, you can add an additional $n$ elements to the new array\n",
    "    - Each of these $n$ elements will provide 2 tokens, one on itself, and one on a token before it\n",
    "    - So by the time you need to double the array again, every element has a token, so every element can be moved\n",
    "\n",
    "- Finally, the amortized cost we need to pay is just the number of tokens we use (i.e. the number of operations we need to conduct to ensure the dynamic resizing works)\n",
    "    - This is simply 3, the number of tokens!\n",
    "    - In other words, the `PushBack` can be done in $O(3)$ time, or constant time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Amortized Cost: Physicist's Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Again, I don't see how this method aids understanding vs the aggregate method, so just stick to that one. Will just explain for completeness\n",
    "\n",
    "- Imagine a potential function $\\phi$\n",
    "\n",
    "- Let the amortized cost of an operation be the actual cost $c_i$ plus the delta in potential between states $h_t$ and $h_{t-1}$, that is \n",
    "$$a_i = c_i + \\phi(h_t) - \\phi(h_{t-1})$$\n",
    "\n",
    "- Note that summing amortized costs across $n$ operations gives us a telescoping sum:\n",
    "$$\\begin{aligned}\n",
    "c_1 + \\phi(h_1) - \\phi(h_{0}) + \\\\\n",
    "c_2 + \\phi(h_2) - \\phi(h_1) + \\\\\n",
    "... c_n + \\phi(h_n) - \\phi(h_{n-1}) \\\\\n",
    "= \\sum_{1}^{n} c_i + \\phi(h_1) - \\phi(h_{n-1})\n",
    "\\end{aligned}$$\n",
    "\n",
    "- So long as we define $\\phi$ such that $\\phi(h_i) - \\phi(h_{i-1}) > 0$, we always get a positive amortized cost for every state change\n",
    "\n",
    "- And all operations above are $O(1)$, so $N$ operations will be $O(N)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PushBack analysis, why must we double array size?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In all the methods we discussed, we assumed that the allocation of the array doubles whenever we hit a capacity. \n",
    "\n",
    "- Why? Strictly speaking, can't we just add some length $k$ to the existing array instead of doubling?\n",
    "\n",
    "- Let's see what happens if we do that:\n",
    "    - Let's suppose we expand by $k=10$ each time we hit a multiple of 10\n",
    "    - Let $c_i$ be the cost of the i-th operation\n",
    "    - $$c_i = 1 + \\begin{Bmatrix} i-1 & \\text{if i-1 is a multiple of 10} \\\\ 0 & \\text{otherwise} \\end{Bmatrix}$$\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\frac{\\sum_{i=1}^{n} c_i}{n} &= \\frac{n + \\sum_{j=1}^{(n-1)/10}10j}{n} \\\\\n",
    "    &= \\frac{n + 10 \\sum_{j=1}^{(n-1)/10} j}{n} \\\\\n",
    "    &= \\frac{n + O(N^2)}{N} & \\text{because } \\sum_0^n i = \\frac{n(n+1)}{2} \\\\\n",
    "    &= O(N)\n",
    "\\end{aligned}$$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
